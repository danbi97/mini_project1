from bs4 import BeautifulSoup
import requests
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
import time
from tqdm.notebook import tqdm

driver = webdriver.Chrome(service=Service('driver/chromedriver'))
driver.get('https://www.saramin.co.kr/zf_user/jobs/list/job-category')

# 경력 - 신입 선택하는 xpath와 기술스택 python 검색 전까지 xpath를 xpath1에 리스트로 만듬
xpath1 = [
    """//*[@id="sp_main_wrapper"]/div[1]/div[1]/button""",
    """//*[@id="sp_main_wrapper"]/div[1]/div[1]/div/div[1]/div[1]/label""",
    """//*[@id="sp_main_wrapper"]/div[1]/div[1]/div/div[3]/button[2]""",
    """//*[@id="sp_main_wrapper"]/div[2]/div/div[1]/div[2]/div[1]/button[6]""",
    """//*[@id="sp_job_category_subDepth_2"]/div[2]/div/div[2]/div/dl[3]/dt/button/span""",
]

# xpath 리스트 값의 xpath를 받아 버튼을 클릭하는 함수
def choose(xpath1):
    for i in xpath1:
        driver.find_element(By.XPATH,i).click()
        time.sleep(1)
        
choose(xpath1) # 함수 호출   

# 검색창에 python을 검색
query_txt = 'python'
element = driver.find_element(By.ID, "job_category_ipt_keyword")
element.send_keys(query_txt)
time.sleep(1)

 # 지역 선택(서울 전체, 경기 전체)하고 검색 버튼 xpath가 담긴 리스트
xpath2 = [
    """//*[@id="autocomplete_has_result"]/div[1]/div[2]/div/ul/li[1]/div/label""",
    """//*[@id="autocomplete_has_result"]/div[3]/button""",
    """//*[@id="sp_main_wrapper"]/div[2]/ul/li[2]/button""",
    """//*[@id="sp_area_lastDepth_101000"]/li[1]/div/label""",
    """//*[@id="depth1_btn_102000"]/button/span[1]""",  
    """//*[@id="search_btn"]"""
]

choose(xpath2) # 함수 호출

# beautifulsoup 이용해서 페이지 전체 긁어서 soup에 넣음
soup = BeautifulSoup(driver.page_source, 'lxml')

# company에 회사명 , link에 기업정보링크 
company1 = soup.select('div.common_recruilt_list div.col.company_nm a span')
company = [elements.get_text() for elements in company1]

link1 = soup.select('div.common_recruilt_list div.col.company_nm a')
link = [elements.get('href') for elements in link1]

# linkname에 공고 제목 가져오기
linkname = soup.select('div.common_recruilt_list div.col.notification_info div.job_tit a span')
linkname = [elements.get_text() for elements in linkname]

# education에 지원 자격 (학력)
education1 = soup.select('div.common_recruilt_list div.col.recruit_condition p.education')
education = [elements.get_text() for elements in education1]

# 지원 마감일 (last_date)
deadlines1 = soup.select('div.common_recruilt_list div.col.support_info p.deadlines')
deadlines = [elements.get_text() for elements in deadlines1] # deadlines에 지원 마감일과 며칠전에 등록했는지 정보가 들어감 (분리 불가)

date_list = [date.split('(') for date in deadlines] # date_list에 deadlines를 ( 로 나눠서 넣음

# 며칠 전 등록된 공고인지 분리
last_date = []

for i in range(len(date_list)):
    if len(date_list[i]) == 3: 
        date_str = '('.join(date_list[i][:2])
    else:
        date_str = date_list[i][0]
    
    last_date.append(date_str)

# place에 근무지 
place1 = soup.select('div.common_recruilt_list div.col.company_info p.work_place')
place = [elements.get_text() for elements in place1]

def join_address(add_list):
    if add_list[1] == "경기":
        # 경기 00시 00구
        word = ' '.join(add_list[1:4])
    else:
        # 서울 00구
        word = ' '.join(add_list[1:3])

    if word[-1] == ',':
        word = word[:-1]
    return word
    
work_place = []
for i,adrress in enumerate(tqdm((place))):
    if adrress == "서울전체" or adrress == "경기전체": # 서울전체나 경기전체로 나와있을경우
        click = 'div.common_recruilt_list div.col.company_nm a' # 공고 링크 클릭
        driver.find_elements(By.CSS_SELECTOR, click)[i].click()
        
        driver.switch_to.window(driver.window_handles[-1]) # 링크 클릭해서 들어갔기 때문에 활성 탭 변경하는 코드
        soup = BeautifulSoup(driver.page_source, 'lxml')
        
        get_place = soup.select('div.jv_cont.jv_summary div.col')[1].get_text().split() # 지역 정보를 가져와서 get_place에 넣음
        
        time.sleep(1)
        
        index_list = [idx for idx in range(len(get_place)) if '근무지역' in get_place[idx]]
        add_list = get_place[index_list[0]+1:]
        work_place.append(join_address(add_list))

        driver.close() # 들어간 공고 링크 탭 닫아주고
        driver.switch_to.window(driver.window_handles[0]) # 활성 탭 변경 
        soup = BeautifulSoup(driver.page_source, 'lxml')
    else:
        work_place.append(adrress)

# 지도에 시각화 
import folium
import json
import googlemaps
import pandas as pd
import numpy as np
import folium.plugins as plug

gmaps_key = "AIzaSyACxDTltkXeEZHmTmtGayzjx7oMJTTvgdA"
gmaps = googlemaps.Client(key = gmaps_key)
gmaps.geocode(work_place, language = 'ko')

station_address = [] # 주소를 담을 리스트
station_lat = [] # 위도를 담을 리스트
station_lng = [] # 경도를 담을 리스트

for name in work_place: # 위에서 가져온 구까지의 주소 정보를 이용해서 위도 경도값 가져옴
        tmp = gmaps.geocode(name, language='ko')
        station_address.append(tmp[0].get('formatted_address')) # 주소 
        tmp_loc = tmp[0].get('geometry') # 위도, 경도
        station_lat.append(tmp_loc['location']['lat'])
        station_lng.append(tmp_loc['location']['lng'])
        
data = {'기업이름': company, '공고제목': linkname, '지원자격': education, 
        '지원마감일': last_date, '링크':link, '지역명': work_place, 'lat': station_lat, 'lng': station_lng }
df = pd.DataFrame(data) # df라는 이름의 공고 정보가 들어간 데이터프레임 생성

map = folium.Map(location = [df['lat'].mean(), df['lng'].mean()], zoom_start = 11)
marker_cluster = plug.MarkerCluster().add_to(map)

for n in df.index:
    folium.Marker([df['lat'][n], df['lng'][n]],
                  popup = "<a href = 'https://www.saramin.co.kr{}' target =_'blink'><pre>{}<br>{}</br></pre><a>".format(df['링크'][n], df['기업이름'][n], df['공고제목'][n]),
                  tooltip = df['기업이름'][n]).add_to(marker_cluster)
    
map
